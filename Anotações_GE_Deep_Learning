{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbrTWM7M3hsU1aKSLELXzJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hevertonvalerio/estudos/blob/main/Anota%C3%A7%C3%B5es_GE_Deep_Learning\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seperação dos grupos"
      ],
      "metadata": {
        "id": "nNfFtAZHtTeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Participação ativa - Pesquisa com materiais para próxima sessão.\n",
        "\n",
        "Roteiro para o econtro. Propor a condução de cada encontro.\n",
        "\n",
        "Exemplos de códigos\n",
        "\n",
        "Alguns alunos propõe e os outros fazem pergunta."
      ],
      "metadata": {
        "id": "Id_975m_tfH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grupo de Introdução à IA"
      ],
      "metadata": {
        "id": "THMF_Z2HwkiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. referencias de trabalho e contexto, ferram.\n",
        "\n",
        "2. eda - Como eles são limpos para uma analise adequada?\n",
        "\n",
        "3. Matematica\n",
        "\n",
        "4. Se funciona ou não\n",
        "\n",
        "5. Algoritimo mais simples\n",
        "\n",
        "6.\n"
      ],
      "metadata": {
        "id": "mhHBK7NbuHWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curso AWS - Machine Learning - ML foundation**\n",
        "\n",
        "Ainda não está disponível - Prof Gustavo Scalabrini"
      ],
      "metadata": {
        "id": "-EKVY3uPwav4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cronograma grupo de Estudos Depp Learning"
      ],
      "metadata": {
        "id": "VCktQ1vfzf6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Biblioteca > Livros eletronicos > \"Minha Biblioteca - biblioteca digital\""
      ],
      "metadata": {
        "id": "jf6evVFhzjLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apresentação Segunda Semana (17/09)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cxg37dh1__i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Estrutura de um neurônio artificial\n"
      ],
      "metadata": {
        "id": "YUqzwlkOOxRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos nos aprofundar na estrutura de um neurônio artificial, que é a base das Redes Neurais Artificiais. Este conceito é fundamental para entender como as redes neurais processam informações e realizam tarefas como classificação, regressão, e reconhecimento de padrões.\n",
        "\n",
        "1. Introdução ao Neurônio Artificial\n",
        "Um neurônio artificial é uma abstração matemática inspirada no neurônio biológico, mas adaptada para processamento computacional. Ele é a menor unidade de uma rede neural e tem a capacidade de aprender padrões a partir de dados, ajustando seus parâmetros internos (pesos e bias) durante o treinamento.\n",
        "\n",
        "2. Componentes de um Neurônio Artificial\n",
        "2.1. Entradas (Inputs)\n",
        "As entradas para um neurônio artificial são valores numéricos que representam características ou atributos dos dados. Por exemplo, em um problema de reconhecimento de dígitos manuscritos, cada pixel de uma imagem pode ser uma entrada. Essas entradas são representadas como um vetor\n",
        "𝑥\n",
        "=\n",
        "[\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "]\n",
        "x=[x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " ], onde cada\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  é uma característica específica.\n",
        "\n",
        "2.2. Pesos (Weights)\n",
        "Cada entrada\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  é associada a um peso\n",
        "𝑤\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        " , que indica a importância relativa daquela entrada no processo de decisão do neurônio. Os pesos são parâmetros aprendidos durante o treinamento e são ajustados para minimizar o erro da rede. A ideia é que entradas mais relevantes para a tarefa tenham pesos maiores, enquanto entradas menos relevantes tenham pesos menores.\n",
        "\n",
        "Matematicamente, os pesos podem ser representados como um vetor\n",
        "𝑤\n",
        "=\n",
        "[\n",
        "𝑤\n",
        "1\n",
        ",\n",
        "𝑤\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑤\n",
        "𝑛\n",
        "]\n",
        "w=[w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,…,w\n",
        "n\n",
        "​\n",
        " ].\n",
        "\n",
        "2.3. Soma Ponderada (Weighted Sum)\n",
        "O neurônio calcula a soma ponderada das entradas, que é essencialmente uma combinação linear das entradas e seus pesos. Essa operação é crucial porque combina as informações das entradas em um único valor, que será processado pela função de ativação.\n",
        "\n",
        "A soma ponderada é dada por:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑤\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        "z=\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " w\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "i\n",
        "​\n",
        " +b\n",
        "Aqui,\n",
        "𝑏\n",
        "b é o bias (viés), um termo adicional que permite ao neurônio ajustar a soma ponderada de maneira a não depender exclusivamente das entradas. O bias ajuda o modelo a se deslocar pela função de ativação, permitindo que a rede modele padrões mais complexos.\n",
        "\n",
        "2.4. Função de Ativação\n",
        "Depois de calcular a soma ponderada, o neurônio passa esse valor através de uma função de ativação. A função de"
      ],
      "metadata": {
        "id": "eL06cjr0NeKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aprofundar nossa compreensão sobre a **estrutura de um neurônio artificial**, que é a unidade fundamental de uma rede neural artificial.\n",
        "\n",
        "### 1.1. Neurônio Artificial: Visão Geral\n",
        "\n",
        "Um neurônio artificial é modelado para simular o comportamento de um neurônio biológico. Ele recebe múltiplos sinais de entrada, processa esses sinais e gera uma saída. Esta saída pode servir como entrada para outros neurônios na rede, formando um sistema interconectado.\n",
        "\n",
        "### 1.2. Componentes do Neurônio Artificial\n",
        "\n",
        "Cada neurônio artificial é composto por três elementos principais:\n",
        "\n",
        "- **Entradas (\\(x_1, x_2, \\dots, x_n\\)):**\n",
        "  - Essas são as características ou dados que o neurônio processa. Cada entrada \\(x_i\\) representa uma característica do dado de entrada.\n",
        "  - Essas entradas são frequentemente valores numéricos, como pixels de uma imagem, medições de sensores, ou características extraídas de textos.\n",
        "\n",
        "- **Pesos (\\(w_1, w_2, \\dots, w_n\\)):**\n",
        "  - Cada entrada é associada a um peso \\(w_i\\), que determina a importância relativa dessa entrada no cálculo final.\n",
        "  - Pesos são parâmetros aprendíveis da rede, ajustados durante o processo de treinamento. Inicialmente, eles podem ser atribuídos aleatoriamente ou com base em algum critério específico.\n",
        "  - Se o peso de uma entrada for alto, essa entrada terá mais influência na saída do neurônio. Um peso negativo pode inverter o efeito de uma entrada positiva.\n",
        "\n",
        "- **Bias (\\(b\\)):**\n",
        "  - O bias é um termo adicional que permite ajustar o output do neurônio independentemente das entradas. Ele ajuda o neurônio a se deslocar da origem, o que é crucial para a capacidade da rede de modelar corretamente os dados.\n",
        "  - Similar aos pesos, o bias também é um parâmetro que é ajustado durante o treinamento.\n",
        "\n",
        "### 1.3. Operação Interna do Neurônio\n",
        "\n",
        "O neurônio processa as entradas da seguinte maneira:\n",
        "\n",
        "#### 1.3.1. Soma Ponderada\n",
        "A primeira operação que o neurônio realiza é calcular a soma ponderada das entradas:\n",
        "\n",
        "\\[\n",
        "z = \\sum_{i=1}^{n} w_i \\cdot x_i + b = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "\\]\n",
        "\n",
        "Aqui, \\(z\\) é o resultado da soma ponderada, que é uma combinação linear das entradas.\n",
        "\n",
        "#### 1.3.2. Função de Ativação\n",
        "Após calcular a soma ponderada, o neurônio passa esse valor através de uma **função de ativação** para introduzir não-linearidade ao modelo. Sem essa não-linearidade, a rede neural não seria capaz de modelar problemas complexos.\n",
        "\n",
        "Algumas funções de ativação comuns incluem:\n",
        "\n",
        "- **ReLU (Rectified Linear Unit):**\n",
        "  \\[\n",
        "  a = \\text{ReLU}(z) = \\max(0, z)\n",
        "  \\]\n",
        "  - A ReLU ativa o neurônio somente quando a soma ponderada é positiva. É muito usada por ser computacionalmente eficiente e ajudar a mitigar o problema do gradiente desaparecendo em redes profundas.\n",
        "\n",
        "- **Sigmoide:**\n",
        "  \\[\n",
        "  a = \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
        "  \\]\n",
        "  - A função sigmoide mapeia o valor \\(z\\) para um intervalo entre 0 e 1, sendo útil para modelos de classificação binária. No entanto, ela pode sofrer com o problema de gradientes pequenos em valores extremos.\n",
        "\n",
        "- **Tanh (Tangente Hiperbólica):**\n",
        "  \\[\n",
        "  a = \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
        "  \\]\n",
        "  - Tanh é similar à função sigmoide, mas mapeia o valor \\(z\\) para um intervalo entre -1 e 1, o que pode ser mais útil em algumas aplicações.\n",
        "\n",
        "### 1.4. Saída do Neurônio\n",
        "\n",
        "A saída do neurônio \\(a\\) é o valor produzido pela função de ativação:\n",
        "\n",
        "\\[\n",
        "a = \\text{função de ativação}(z)\n",
        "\\]\n",
        "\n",
        "Este valor \\(a\\) pode ser usado como a saída final do neurônio ou servir como entrada para outros neurônios em camadas subsequentes da rede neural.\n",
        "\n",
        "### 1.5. Interpretação e Treinamento\n",
        "\n",
        "No contexto do treinamento de redes neurais, o objetivo é ajustar os pesos \\(w_i\\) e o bias \\(b\\) para que a saída do neurônio esteja o mais próxima possível do valor desejado (target). Isso é feito usando algoritmos de otimização, como o Gradient Descent, que minimizam uma função de perda, representando o erro da rede.\n",
        "\n",
        "### 1.6. Importância da Estrutura do Neurônio\n",
        "\n",
        "A estrutura do neurônio artificial, embora simples, permite que redes neurais modelam relações complexas entre entradas e saídas. Ao empilhar muitos neurônios e camadas, as redes podem capturar padrões e regularidades em dados complexos, como imagens, texto e séries temporais.\n",
        "\n",
        "Essa compreensão detalhada da estrutura do neurônio artificial é essencial para entender como redes neurais mais complexas funcionam e como podemos treiná-las para resolver problemas reais."
      ],
      "metadata": {
        "id": "yiW7VcZ-Nyz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos nos aprofundar na **estrutura de um neurônio artificial**, um componente essencial das Redes Neurais Artificiais. Este conceito é inspirado no neurônio biológico, mas traduzido para o domínio matemático e computacional.\n",
        "\n",
        "### Estrutura de um Neurônio Artificial\n",
        "\n",
        "#### 1. **Entradas (Inputs) e Pesos (Weights)**\n",
        "\n",
        "Cada neurônio artificial recebe múltiplas entradas, que podem ser denotadas como \\(x_1, x_2, \\dots, x_n\\). Essas entradas representam características ou dados que o modelo está processando. Para cada entrada, existe um peso associado (\\(w_1, w_2, \\dots, w_n\\)). Esses pesos são valores numéricos que determinam a importância de cada entrada.\n",
        "\n",
        "**Exemplo Prático:**\n",
        "Imagine que você está desenvolvendo um modelo de previsão de preços de casas, onde as entradas são características como tamanho da casa, número de quartos e localização. O neurônio receberia esses valores como entradas, e os pesos determinariam quão importante cada uma dessas características é para prever o preço.\n",
        "\n",
        "Matematicamente, a combinação das entradas e pesos é expressa como uma soma ponderada:\n",
        "\\[\n",
        "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "\\]\n",
        "Aqui, \\(b\\) representa o **termo de bias** (viés), que é um valor adicional que permite que o neurônio se ajuste de maneira mais flexível aos dados, movendo a função de ativação para frente ou para trás no eixo de decisão.\n",
        "\n",
        "#### 2. **Função de Soma (Weighted Sum)**\n",
        "\n",
        "O valor \\(z\\) obtido é a soma ponderada das entradas e pesos, incluindo o bias. Esta soma ponderada é o que o neurônio processa antes de tomar qualquer \"decisão\" sobre a saída.\n",
        "\n",
        "A função de soma é essencial porque integra todas as entradas de forma linear, combinando-as em um único valor que representa o estado atual das informações processadas pelo neurônio. Este valor \\(z\\) será então transformado por uma função de ativação.\n",
        "\n",
        "#### 3. **Função de Ativação (Activation Function)**\n",
        "\n",
        "Após calcular a soma ponderada, o neurônio passa este valor \\(z\\) por uma função de ativação. A função de ativação é crucial porque ela introduz **não-linearidade** ao sistema. Sem ela, a rede neural seria equivalente a uma simples regressão linear, incapaz de capturar relações complexas nos dados.\n",
        "\n",
        "As funções de ativação mais comuns incluem:\n",
        "\n",
        "- **ReLU (Rectified Linear Unit):** \\( \\text{ReLU}(z) = \\max(0, z) \\)  \n",
        "  A ReLU é popular porque resolve o problema do desvanecimento do gradiente (gradient vanishing) em redes profundas e é eficiente computacionalmente. Ela transforma valores negativos em zero e mantém os valores positivos inalterados.\n",
        "\n",
        "- **Sigmoide:** \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\)  \n",
        "  A função sigmoide é usada principalmente em problemas de classificação binária. Ela mapeia a soma ponderada para um valor entre 0 e 1, interpretável como uma probabilidade.\n",
        "\n",
        "- **Tanh (Tangente Hiperbólica):** \\( \\text{Tanh}(z) = \\frac{2}{1 + e^{-2z}} - 1 \\)  \n",
        "  A tanh é semelhante à sigmoide, mas mapeia o valor de \\(z\\) para um intervalo entre -1 e 1. É útil quando se quer centrar os dados em torno de zero.\n",
        "\n",
        "A saída do neurônio após a aplicação da função de ativação é:\n",
        "\\[\n",
        "a = \\text{função de ativação}(z)\n",
        "\\]\n",
        "onde \\(a\\) é a saída do neurônio, pronta para ser transmitida para a próxima camada na rede ou para gerar uma predição final, dependendo de onde o neurônio se encontra na arquitetura da rede.\n",
        "\n",
        "#### 4. **Interpretação e Papel do Neurônio Artificial**\n",
        "\n",
        "O neurônio artificial, em essência, toma uma decisão com base nas entradas que recebe e nos pesos associados a essas entradas. Ele processa as informações através da função de soma e decide, através da função de ativação, se deve \"ativar\" (ou seja, gerar uma saída significativa).\n",
        "\n",
        "Em uma rede neural, múltiplos neurônios operam juntos, cada um processando suas respectivas entradas. O poder das redes neurais vem da capacidade de compor várias camadas de neurônios, cada uma capturando diferentes níveis de abstração dos dados. Em camadas iniciais, os neurônios podem identificar padrões simples (como bordas em uma imagem), enquanto camadas mais profundas identificam padrões complexos (como formas ou objetos inteiros).\n",
        "\n",
        "### Visualizando o Neurônio Artificial\n",
        "\n",
        "Imagine o neurônio como um filtro que recebe várias informações, pesa cada uma, soma todas e decide, através de uma função matemática, qual será sua resposta. Se pensarmos em termos de uma rede neural complexa, cada neurônio individualmente é responsável por uma pequena parte do trabalho, mas todos juntos formam um sistema poderoso capaz de aprender e fazer predições a partir dos dados.\n",
        "\n",
        "Essa estrutura básica do neurônio artificial é o que permite que redes neurais, desde as mais simples até as mais profundas e complexas, sejam capazes de realizar tarefas como reconhecimento de imagem, tradução de linguagem e até mesmo conduzir carros autonomamente."
      ],
      "metadata": {
        "id": "plz-vCjPOsgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Multilayer Perceptron (MLP)\"\n"
      ],
      "metadata": {
        "id": "6wHuAIdPmikD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Vamos aprofundar o conceito de **Multilayer Perceptron (MLP)**, uma das arquiteturas mais básicas e fundamentais nas redes neurais artificiais. O MLP é conhecido por ser uma rede neural totalmente conectada (fully connected) e é frequentemente usado para problemas de classificação e regressão. Seu design permite que ele capture padrões complexos nos dados, graças à sua arquitetura de múltiplas camadas.\n",
        "\n",
        "### Multilayer Perceptron (MLP)\n",
        "\n",
        "#### 1. **Estrutura do MLP**\n",
        "\n",
        "O MLP é composto por várias camadas de neurônios artificiais, organizadas em:\n",
        "\n",
        "- **Camada de Entrada (Input Layer):** Esta camada recebe os dados diretamente. Cada neurônio na camada de entrada corresponde a uma característica ou variável dos dados de entrada. Por exemplo, se estamos lidando com um conjunto de dados com três características (como altura, peso e idade), a camada de entrada terá três neurônios.\n",
        "\n",
        "- **Camadas Ocultas (Hidden Layers):** As camadas intermediárias entre a camada de entrada e a camada de saída. O termo \"oculta\" refere-se ao fato de que os valores dessas camadas não são visíveis externamente; elas servem apenas para processar e transformar os dados. Um MLP pode ter uma ou várias camadas ocultas, e a profundidade da rede depende do número de camadas. Cada neurônio em uma camada oculta realiza a operação de soma ponderada das saídas da camada anterior, passa essa soma por uma função de ativação e envia o resultado para a próxima camada.\n",
        "\n",
        "- **Camada de Saída (Output Layer):** A última camada do MLP gera a predição final. O número de neurônios na camada de saída depende da tarefa. Para uma tarefa de classificação binária, há geralmente um neurônio, enquanto que para classificação multiclasse, o número de neurônios corresponde ao número de classes. Para regressão, a camada de saída geralmente tem um neurônio.\n",
        "\n",
        "**Arquitetura Típica:**\n",
        "- **Número de neurônios na camada de entrada:** Depende das características do dataset (número de variáveis de entrada).\n",
        "- **Número de camadas ocultas e neurônios por camada:** Isso é ajustável, e é uma parte crítica do design de redes neurais. Mais camadas e neurônios permitem que a rede aprenda representações mais complexas, mas podem também aumentar a chance de overfitting (sobreajuste).\n",
        "- **Número de neurônios na camada de saída:** Depende da tarefa. Para problemas de classificação binária, há tipicamente um único neurônio com uma função de ativação sigmoide para gerar um valor entre 0 e 1, interpretado como probabilidade.\n",
        "\n",
        "#### 2. **Conexões entre as Camadas**\n",
        "\n",
        "Uma característica importante do MLP é que ele é uma rede neural **totalmente conectada**. Isso significa que:\n",
        "\n",
        "- Cada neurônio em uma camada está conectado a todos os neurônios da camada seguinte.\n",
        "- Cada conexão entre os neurônios tem um peso associado, que será ajustado durante o treinamento.\n",
        "\n",
        "Essa interconectividade garante que o MLP possa aprender padrões complexos nos dados, pois cada neurônio recebe informações de todos os neurônios da camada anterior. No entanto, isso também significa que o número de parâmetros aumenta rapidamente com o aumento do número de neurônios e camadas.\n",
        "\n",
        "#### 3. **Processo de Treinamento no MLP**\n",
        "\n",
        "O processo de treinamento de um MLP envolve dois passos principais: **Forward Propagation** e **Backpropagation** (estes serão detalhados em outras partes, mas aqui está uma visão geral).\n",
        "\n",
        "- **Forward Propagation:** Durante a fase de forward propagation, as entradas são passadas pela rede, camada por camada, até gerar uma predição. Em cada camada, a soma ponderada das entradas é calculada, e o resultado é passado por uma função de ativação (como ReLU, sigmoide ou tanh).\n",
        "\n",
        "- **Backpropagation:** Após o forward propagation, o erro entre a predição da rede e o valor real é calculado usando uma função de perda (loss function). Este erro é propagado de volta através da rede, e os pesos são ajustados para minimizar o erro, usando um algoritmo de otimização, como o gradient descent (descida do gradiente). Esse processo é repetido várias vezes (épocas) até que o modelo aprenda a fazer boas predições.\n",
        "\n",
        "#### 4. **Funções de Ativação nas Camadas Ocultas e de Saída**\n",
        "\n",
        "- **Funções de Ativação nas Camadas Ocultas:** O MLP usa funções de ativação não lineares (como ReLU, sigmoide ou tanh) nas camadas ocultas para introduzir não-linearidade. Isso permite que a rede aprenda relações complexas que não podem ser modeladas por uma combinação linear simples de pesos e entradas.\n",
        "\n",
        "- **Função de Ativação na Camada de Saída:** A escolha da função de ativação na camada de saída depende do tipo de problema:\n",
        "  - **Classificação Binária:** Usualmente, a função sigmoide é usada, pois ela mapeia o valor da saída para um intervalo entre 0 e 1, representando uma probabilidade.\n",
        "  - **Classificação Multiclasse:** Geralmente, a função **softmax** é usada, que mapeia a saída para uma distribuição de probabilidade sobre múltiplas classes.\n",
        "  - **Regressão:** Para problemas de regressão, não é necessário usar uma função de ativação na camada de saída, pois o objetivo é prever um valor contínuo.\n",
        "\n",
        "#### 5. **Capacidade de Aprendizado do MLP**\n",
        "\n",
        "O poder de um MLP está em sua capacidade de aprender representações a diferentes níveis de abstração:\n",
        "\n",
        "- **Camadas Superficiais:** Capturam padrões mais simples e diretos dos dados. Por exemplo, em uma rede treinada para reconhecer dígitos, as primeiras camadas podem identificar bordas e curvas.\n",
        "  \n",
        "- **Camadas Profundas:** Com múltiplas camadas ocultas, o MLP pode aprender a combinar esses padrões simples em padrões mais complexos. Em um exemplo de reconhecimento de imagem, as camadas mais profundas podem aprender a reconhecer formas inteiras, como números ou letras.\n",
        "\n",
        "Embora o MLP tenha uma arquitetura poderosa, ele possui algumas limitações:\n",
        "- **Escalabilidade:** Para problemas de alta dimensão (como imagens), o número de parâmetros cresce rapidamente, tornando o treinamento mais desafiador.\n",
        "- **Inabilidade de Capturar Padrões Sequenciais:** O MLP não leva em consideração a ordem sequencial dos dados, o que limita sua eficácia para problemas como modelagem de séries temporais ou processamento de linguagem natural. Para esses problemas, arquiteturas como redes neurais recorrentes (RNNs) ou convolucionais (CNNs) são mais apropriadas.\n",
        "\n",
        "#### 6. **Overfitting e Regularização**\n",
        "\n",
        "Com grandes quantidades de neurônios e camadas ocultas, um MLP pode facilmente \"memorizar\" os dados de treino, levando a um problema conhecido como **overfitting**, onde o modelo tem desempenho excelente nos dados de treinamento, mas falha em generalizar para novos dados.\n",
        "\n",
        "- **Regularização:** Para combater o overfitting, técnicas como **dropout** (desligar aleatoriamente alguns neurônios durante o treinamento) e **penalidades de regularização L2** (que restringem o crescimento dos pesos) são frequentemente usadas. Essas técnicas ajudam o MLP a se generalizar melhor para novos dados, reduzindo a chance de memorizar ruídos específicos dos dados de treinamento.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "O **Multilayer Perceptron** é um modelo robusto e flexível, capaz de aprender padrões complexos nos dados, graças à sua arquitetura de múltiplas camadas. Embora tenha limitações para dados com alta dimensionalidade e dados sequenciais, o MLP continua sendo um ponto de partida essencial para o aprendizado profundo, especialmente em problemas de classificação e regressão."
      ],
      "metadata": {
        "id": "QJAo8zj7mYsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Foward e BackWard propagation"
      ],
      "metadata": {
        "id": "MH1si2sIO1l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos nos aprofundar nos conceitos de **forward propagation** e **backpropagation**, que são processos fundamentais para o treinamento de redes neurais, incluindo o **Multilayer Perceptron (MLP)**. Eles permitem que o modelo aprenda a partir dos dados ajustando os pesos de maneira eficiente, otimizando as predições realizadas pela rede neural.\n",
        "\n",
        "### Forward Propagation\n",
        "\n",
        "#### 1. **Definição e Funcionamento**\n",
        "O **forward propagation** é o processo no qual os dados de entrada percorrem a rede neural, passando de uma camada para a outra, até chegar à camada de saída. A rede realiza cálculos em cada neurônio das camadas ocultas e na camada de saída, gerando uma predição final.\n",
        "\n",
        "Em termos simples, este processo envolve:\n",
        "\n",
        "1. Multiplicar os valores de entrada pelos pesos associados a cada conexão.\n",
        "2. Somar o resultado para obter a soma ponderada.\n",
        "3. Adicionar o bias (termo de viés) à soma ponderada.\n",
        "4. Passar a soma ponderada por uma função de ativação, para introduzir não-linearidade.\n",
        "5. Repetir este processo para cada camada até chegar à camada de saída.\n",
        "\n",
        "##### Exemplo Prático:\n",
        "Considere um problema simples de classificação binária, onde queremos prever se uma pessoa tem ou não uma determinada doença com base em três características: idade, peso e pressão arterial. A entrada (\\( x_1, x_2, x_3 \\)) corresponde a essas três características, os pesos (\\( w_1, w_2, w_3 \\)) indicam a importância de cada uma, e o processo de forward propagation vai calcular a predição da probabilidade de doença.\n",
        "\n",
        "1. Para cada neurônio, a soma ponderada \\( z \\) é calculada como:\n",
        "   \\[\n",
        "   z = w_1x_1 + w_2x_2 + w_3x_3 + b\n",
        "   \\]\n",
        "2. O valor \\( z \\) é passado por uma função de ativação (como ReLU ou sigmoide) para gerar a saída \\( a \\) do neurônio:\n",
        "   \\[\n",
        "   a = \\sigma(z)\n",
        "   \\]\n",
        "3. Esse processo se repete para cada camada da rede até a camada de saída, onde o valor final é a predição feita pela rede.\n",
        "\n",
        "#### 2. **Predição Final**\n",
        "Na camada de saída, a predição final é gerada. Para problemas de classificação binária, essa predição pode ser um valor entre 0 e 1, interpretado como a probabilidade de um evento (por exemplo, se uma pessoa tem a doença). Para problemas de classificação multiclasse, a função softmax pode ser usada para gerar probabilidades para cada classe.\n",
        "\n",
        "#### 3. **Limitações do Forward Propagation**\n",
        "No forward propagation, não há ajuste nos pesos da rede. Ele simplesmente calcula a saída baseada nos pesos atuais, que são inicialmente aleatórios. Portanto, o processo por si só não faz a rede aprender; é apenas uma fase de avaliação. O aprendizado real ocorre durante o **backpropagation**, que ajusta os pesos para melhorar as predições futuras.\n",
        "\n",
        "---\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "#### 1. **Definição e Funcionamento**\n",
        "O **backpropagation** (retropropagação) é o processo no qual o erro calculado na predição final é propagado de volta através da rede para ajustar os pesos. Isso permite que a rede melhore suas predições ao longo do tempo, reduzindo o erro.\n",
        "\n",
        "O objetivo do backpropagation é minimizar uma função de perda, como o erro quadrático médio (MSE) ou a entropia cruzada, dependendo do tipo de problema (regressão ou classificação).\n",
        "\n",
        "#### 2. **Etapas do Backpropagation**\n",
        "\n",
        "O processo de backpropagation envolve as seguintes etapas:\n",
        "\n",
        "##### 1. **Cálculo do Erro na Camada de Saída**\n",
        "Depois que o forward propagation é completado, a predição gerada pela rede \\( \\hat{y} \\) é comparada com o valor real \\( y \\). A diferença entre \\( \\hat{y} \\) e \\( y \\) é o **erro** da rede.\n",
        "\n",
        "Para um problema de regressão, podemos usar uma função de perda como o erro quadrático médio:\n",
        "\\[\n",
        "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
        "\\]\n",
        "\n",
        "##### 2. **Cálculo do Gradiente**\n",
        "O erro calculado é usado para determinar o quanto cada peso na rede contribuiu para esse erro. Isso é feito calculando o **gradiente** da função de perda em relação aos pesos, usando a **regra da cadeia**.\n",
        "\n",
        "A regra da cadeia é uma técnica de cálculo que permite calcular a derivada de uma função composta. Como os valores de saída dependem dos pesos e dos valores de ativação, que por sua vez dependem dos pesos da camada anterior, precisamos calcular essas derivadas de forma sequencial.\n",
        "\n",
        "O gradiente é a inclinação da função de perda em relação a um determinado peso, ou seja, ele indica o quanto a função de perda mudará se ajustarmos ligeiramente esse peso. O objetivo é ajustar os pesos na direção que minimize a perda.\n",
        "\n",
        "##### 3. **Ajuste dos Pesos**\n",
        "Os pesos são atualizados com base nos gradientes calculados. Isso é feito usando um algoritmo de otimização como a **descida do gradiente** (gradient descent). O peso \\( w \\) é atualizado pela regra:\n",
        "\\[\n",
        "w_{novo} = w_{atual} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
        "\\]\n",
        "onde \\( \\eta \\) é a **taxa de aprendizado** (learning rate) e \\( \\frac{\\partial \\text{Loss}}{\\partial w} \\) é o gradiente da função de perda em relação ao peso \\( w \\).\n",
        "\n",
        "Esse processo é repetido para cada peso na rede, ajustando-os na direção que reduz o erro da predição.\n",
        "\n",
        "##### 4. **Propagação do Erro para Camadas Anteriores**\n",
        "O erro da camada de saída é propagado para as camadas ocultas. O erro em uma camada oculta é calculado com base nos erros das camadas subsequentes. Isso permite que o modelo ajuste os pesos em todas as camadas, não apenas na camada de saída.\n",
        "\n",
        "#### 3. **Algoritmo de Descida do Gradiente**\n",
        "O algoritmo de descida do gradiente é um dos métodos mais comuns para ajustar os pesos. Ele trabalha iterativamente para reduzir o erro ajustando os pesos em pequenas quantidades. Dependendo da abordagem, existem algumas variações da descida do gradiente:\n",
        "- **Batch Gradient Descent:** Calcula o gradiente em todo o conjunto de dados de treinamento antes de atualizar os pesos.\n",
        "- **Stochastic Gradient Descent (SGD):** Atualiza os pesos após cada exemplo de treino, o que torna o processo mais rápido, mas com mais variação.\n",
        "- **Mini-batch Gradient Descent:** Uma combinação das duas anteriores, atualizando os pesos após processar pequenos lotes de exemplos.\n",
        "\n",
        "Além disso, pode-se usar variantes como **Adam**, **RMSProp**, e **Momentum**, que adaptam dinamicamente a taxa de aprendizado ou aceleram a convergência.\n",
        "\n",
        "---\n",
        "\n",
        "### Relação entre Forward Propagation e Backpropagation\n",
        "\n",
        "O forward propagation e o backpropagation trabalham juntos no treinamento de redes neurais:\n",
        "\n",
        "1. **Forward Propagation:** Transforma os dados de entrada em uma predição. Durante essa fase, as ativações de cada camada são armazenadas, para que possam ser usadas no cálculo do gradiente na fase de backpropagation.\n",
        "  \n",
        "2. **Backpropagation:** Com base na predição gerada e no erro calculado, os pesos são ajustados para reduzir o erro em futuras predições.\n",
        "\n",
        "Este processo de forward e backward é repetido por muitas iterações (épocas), até que a rede neural converja para uma solução que minimize a função de perda e maximize o desempenho nas predições.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "**Forward propagation** é o processo de calcular a saída de uma rede neural a partir das entradas, enquanto **backpropagation** ajusta os pesos da rede para reduzir o erro nas predições futuras. A combinação desses dois processos é o coração do treinamento de redes neurais, permitindo que modelos aprendam com dados e melhorem seu desempenho ao longo do tempo."
      ],
      "metadata": {
        "id": "DG83XDIQmUo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10/09\n"
      ],
      "metadata": {
        "id": "3xUl-vpmaGPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mWMP5p2QaIDh"
      }
    }
  ]
}