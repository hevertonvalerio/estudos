{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu+JeH5NZyNQEhyG4PGui5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hevertonvalerio/estudos/blob/main/Anota%C3%A7%C3%B5es_GE_Deep_Learning_e_Intro_a_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sepera√ß√£o dos grupos"
      ],
      "metadata": {
        "id": "nNfFtAZHtTeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Participa√ß√£o ativa - Pesquisa com materiais para pr√≥xima sess√£o.\n",
        "\n",
        "Roteiro para o econtro. Propor a condu√ß√£o de cada encontro.\n",
        "\n",
        "Exemplos de c√≥digos\n",
        "\n",
        "Alguns alunos prop√µe e os outros fazem pergunta."
      ],
      "metadata": {
        "id": "Id_975m_tfH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grupo de Introdu√ß√£o √† IA"
      ],
      "metadata": {
        "id": "THMF_Z2HwkiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. referencias de trabalho e contexto, ferram.\n",
        "\n",
        "2. eda - Como eles s√£o limpos para uma analise adequada?\n",
        "\n",
        "3. Matematica\n",
        "\n",
        "4. Se funciona ou n√£o\n",
        "\n",
        "5. Algoritimo mais simples\n",
        "\n",
        "6.\n"
      ],
      "metadata": {
        "id": "mhHBK7NbuHWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curso AWS - Machine Learning - ML foundation**\n",
        "\n",
        "Ainda n√£o est√° dispon√≠vel - Prof Gustavo Scalabrini"
      ],
      "metadata": {
        "id": "-EKVY3uPwav4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cronograma grupo de Estudos Depp Learning"
      ],
      "metadata": {
        "id": "VCktQ1vfzf6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Biblioteca > Livros eletronicos > \"Minha Biblioteca - biblioteca digital\""
      ],
      "metadata": {
        "id": "jf6evVFhzjLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apresenta√ß√£o Segunda Semana (17/09)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cxg37dh1__i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Estrutura de um neur√¥nio artificial\n"
      ],
      "metadata": {
        "id": "YUqzwlkOOxRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos nos aprofundar na estrutura de um neur√¥nio artificial, que √© a base das Redes Neurais Artificiais. Este conceito √© fundamental para entender como as redes neurais processam informa√ß√µes e realizam tarefas como classifica√ß√£o, regress√£o, e reconhecimento de padr√µes.\n",
        "\n",
        "1. Introdu√ß√£o ao Neur√¥nio Artificial\n",
        "Um neur√¥nio artificial √© uma abstra√ß√£o matem√°tica inspirada no neur√¥nio biol√≥gico, mas adaptada para processamento computacional. Ele √© a menor unidade de uma rede neural e tem a capacidade de aprender padr√µes a partir de dados, ajustando seus par√¢metros internos (pesos e bias) durante o treinamento.\n",
        "\n",
        "2. Componentes de um Neur√¥nio Artificial\n",
        "2.1. Entradas (Inputs)\n",
        "As entradas para um neur√¥nio artificial s√£o valores num√©ricos que representam caracter√≠sticas ou atributos dos dados. Por exemplo, em um problema de reconhecimento de d√≠gitos manuscritos, cada pixel de uma imagem pode ser uma entrada. Essas entradas s√£o representadas como um vetor\n",
        "ùë•\n",
        "=\n",
        "[\n",
        "ùë•\n",
        "1\n",
        ",\n",
        "ùë•\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë•\n",
        "ùëõ\n",
        "]\n",
        "x=[x\n",
        "1\n",
        "‚Äã\n",
        " ,x\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶,x\n",
        "n\n",
        "‚Äã\n",
        " ], onde cada\n",
        "ùë•\n",
        "ùëñ\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        "  √© uma caracter√≠stica espec√≠fica.\n",
        "\n",
        "2.2. Pesos (Weights)\n",
        "Cada entrada\n",
        "ùë•\n",
        "ùëñ\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        "  √© associada a um peso\n",
        "ùë§\n",
        "ùëñ\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " , que indica a import√¢ncia relativa daquela entrada no processo de decis√£o do neur√¥nio. Os pesos s√£o par√¢metros aprendidos durante o treinamento e s√£o ajustados para minimizar o erro da rede. A ideia √© que entradas mais relevantes para a tarefa tenham pesos maiores, enquanto entradas menos relevantes tenham pesos menores.\n",
        "\n",
        "Matematicamente, os pesos podem ser representados como um vetor\n",
        "ùë§\n",
        "=\n",
        "[\n",
        "ùë§\n",
        "1\n",
        ",\n",
        "ùë§\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë§\n",
        "ùëõ\n",
        "]\n",
        "w=[w\n",
        "1\n",
        "‚Äã\n",
        " ,w\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶,w\n",
        "n\n",
        "‚Äã\n",
        " ].\n",
        "\n",
        "2.3. Soma Ponderada (Weighted Sum)\n",
        "O neur√¥nio calcula a soma ponderada das entradas, que √© essencialmente uma combina√ß√£o linear das entradas e seus pesos. Essa opera√ß√£o √© crucial porque combina as informa√ß√µes das entradas em um √∫nico valor, que ser√° processado pela fun√ß√£o de ativa√ß√£o.\n",
        "\n",
        "A soma ponderada √© dada por:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùë§\n",
        "ùëñ\n",
        "‚ãÖ\n",
        "ùë•\n",
        "ùëñ\n",
        "+\n",
        "ùëè\n",
        "z=\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " w\n",
        "i\n",
        "‚Äã\n",
        " ‚ãÖx\n",
        "i\n",
        "‚Äã\n",
        " +b\n",
        "Aqui,\n",
        "ùëè\n",
        "b √© o bias (vi√©s), um termo adicional que permite ao neur√¥nio ajustar a soma ponderada de maneira a n√£o depender exclusivamente das entradas. O bias ajuda o modelo a se deslocar pela fun√ß√£o de ativa√ß√£o, permitindo que a rede modele padr√µes mais complexos.\n",
        "\n",
        "2.4. Fun√ß√£o de Ativa√ß√£o\n",
        "Depois de calcular a soma ponderada, o neur√¥nio passa esse valor atrav√©s de uma fun√ß√£o de ativa√ß√£o. A fun√ß√£o de"
      ],
      "metadata": {
        "id": "eL06cjr0NeKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aprofundar nossa compreens√£o sobre a **estrutura de um neur√¥nio artificial**, que √© a unidade fundamental de uma rede neural artificial.\n",
        "\n",
        "### 1.1. Neur√¥nio Artificial: Vis√£o Geral\n",
        "\n",
        "Um neur√¥nio artificial √© modelado para simular o comportamento de um neur√¥nio biol√≥gico. Ele recebe m√∫ltiplos sinais de entrada, processa esses sinais e gera uma sa√≠da. Esta sa√≠da pode servir como entrada para outros neur√¥nios na rede, formando um sistema interconectado.\n",
        "\n",
        "### 1.2. Componentes do Neur√¥nio Artificial\n",
        "\n",
        "Cada neur√¥nio artificial √© composto por tr√™s elementos principais:\n",
        "\n",
        "- **Entradas (\\(x_1, x_2, \\dots, x_n\\)):**\n",
        "  - Essas s√£o as caracter√≠sticas ou dados que o neur√¥nio processa. Cada entrada \\(x_i\\) representa uma caracter√≠stica do dado de entrada.\n",
        "  - Essas entradas s√£o frequentemente valores num√©ricos, como pixels de uma imagem, medi√ß√µes de sensores, ou caracter√≠sticas extra√≠das de textos.\n",
        "\n",
        "- **Pesos (\\(w_1, w_2, \\dots, w_n\\)):**\n",
        "  - Cada entrada √© associada a um peso \\(w_i\\), que determina a import√¢ncia relativa dessa entrada no c√°lculo final.\n",
        "  - Pesos s√£o par√¢metros aprend√≠veis da rede, ajustados durante o processo de treinamento. Inicialmente, eles podem ser atribu√≠dos aleatoriamente ou com base em algum crit√©rio espec√≠fico.\n",
        "  - Se o peso de uma entrada for alto, essa entrada ter√° mais influ√™ncia na sa√≠da do neur√¥nio. Um peso negativo pode inverter o efeito de uma entrada positiva.\n",
        "\n",
        "- **Bias (\\(b\\)):**\n",
        "  - O bias √© um termo adicional que permite ajustar o output do neur√¥nio independentemente das entradas. Ele ajuda o neur√¥nio a se deslocar da origem, o que √© crucial para a capacidade da rede de modelar corretamente os dados.\n",
        "  - Similar aos pesos, o bias tamb√©m √© um par√¢metro que √© ajustado durante o treinamento.\n",
        "\n",
        "### 1.3. Opera√ß√£o Interna do Neur√¥nio\n",
        "\n",
        "O neur√¥nio processa as entradas da seguinte maneira:\n",
        "\n",
        "#### 1.3.1. Soma Ponderada\n",
        "A primeira opera√ß√£o que o neur√¥nio realiza √© calcular a soma ponderada das entradas:\n",
        "\n",
        "\\[\n",
        "z = \\sum_{i=1}^{n} w_i \\cdot x_i + b = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "\\]\n",
        "\n",
        "Aqui, \\(z\\) √© o resultado da soma ponderada, que √© uma combina√ß√£o linear das entradas.\n",
        "\n",
        "#### 1.3.2. Fun√ß√£o de Ativa√ß√£o\n",
        "Ap√≥s calcular a soma ponderada, o neur√¥nio passa esse valor atrav√©s de uma **fun√ß√£o de ativa√ß√£o** para introduzir n√£o-linearidade ao modelo. Sem essa n√£o-linearidade, a rede neural n√£o seria capaz de modelar problemas complexos.\n",
        "\n",
        "Algumas fun√ß√µes de ativa√ß√£o comuns incluem:\n",
        "\n",
        "- **ReLU (Rectified Linear Unit):**\n",
        "  \\[\n",
        "  a = \\text{ReLU}(z) = \\max(0, z)\n",
        "  \\]\n",
        "  - A ReLU ativa o neur√¥nio somente quando a soma ponderada √© positiva. √â muito usada por ser computacionalmente eficiente e ajudar a mitigar o problema do gradiente desaparecendo em redes profundas.\n",
        "\n",
        "- **Sigmoide:**\n",
        "  \\[\n",
        "  a = \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
        "  \\]\n",
        "  - A fun√ß√£o sigmoide mapeia o valor \\(z\\) para um intervalo entre 0 e 1, sendo √∫til para modelos de classifica√ß√£o bin√°ria. No entanto, ela pode sofrer com o problema de gradientes pequenos em valores extremos.\n",
        "\n",
        "- **Tanh (Tangente Hiperb√≥lica):**\n",
        "  \\[\n",
        "  a = \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
        "  \\]\n",
        "  - Tanh √© similar √† fun√ß√£o sigmoide, mas mapeia o valor \\(z\\) para um intervalo entre -1 e 1, o que pode ser mais √∫til em algumas aplica√ß√µes.\n",
        "\n",
        "### 1.4. Sa√≠da do Neur√¥nio\n",
        "\n",
        "A sa√≠da do neur√¥nio \\(a\\) √© o valor produzido pela fun√ß√£o de ativa√ß√£o:\n",
        "\n",
        "\\[\n",
        "a = \\text{fun√ß√£o de ativa√ß√£o}(z)\n",
        "\\]\n",
        "\n",
        "Este valor \\(a\\) pode ser usado como a sa√≠da final do neur√¥nio ou servir como entrada para outros neur√¥nios em camadas subsequentes da rede neural.\n",
        "\n",
        "### 1.5. Interpreta√ß√£o e Treinamento\n",
        "\n",
        "No contexto do treinamento de redes neurais, o objetivo √© ajustar os pesos \\(w_i\\) e o bias \\(b\\) para que a sa√≠da do neur√¥nio esteja o mais pr√≥xima poss√≠vel do valor desejado (target). Isso √© feito usando algoritmos de otimiza√ß√£o, como o Gradient Descent, que minimizam uma fun√ß√£o de perda, representando o erro da rede.\n",
        "\n",
        "### 1.6. Import√¢ncia da Estrutura do Neur√¥nio\n",
        "\n",
        "A estrutura do neur√¥nio artificial, embora simples, permite que redes neurais modelam rela√ß√µes complexas entre entradas e sa√≠das. Ao empilhar muitos neur√¥nios e camadas, as redes podem capturar padr√µes e regularidades em dados complexos, como imagens, texto e s√©ries temporais.\n",
        "\n",
        "Essa compreens√£o detalhada da estrutura do neur√¥nio artificial √© essencial para entender como redes neurais mais complexas funcionam e como podemos trein√°-las para resolver problemas reais."
      ],
      "metadata": {
        "id": "yiW7VcZ-Nyz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos nos aprofundar na **estrutura de um neur√¥nio artificial**, um componente essencial das Redes Neurais Artificiais. Este conceito √© inspirado no neur√¥nio biol√≥gico, mas traduzido para o dom√≠nio matem√°tico e computacional.\n",
        "\n",
        "### Estrutura de um Neur√¥nio Artificial\n",
        "\n",
        "#### 1. **Entradas (Inputs) e Pesos (Weights)**\n",
        "\n",
        "Cada neur√¥nio artificial recebe m√∫ltiplas entradas, que podem ser denotadas como \\(x_1, x_2, \\dots, x_n\\). Essas entradas representam caracter√≠sticas ou dados que o modelo est√° processando. Para cada entrada, existe um peso associado (\\(w_1, w_2, \\dots, w_n\\)). Esses pesos s√£o valores num√©ricos que determinam a import√¢ncia de cada entrada.\n",
        "\n",
        "**Exemplo Pr√°tico:**\n",
        "Imagine que voc√™ est√° desenvolvendo um modelo de previs√£o de pre√ßos de casas, onde as entradas s√£o caracter√≠sticas como tamanho da casa, n√∫mero de quartos e localiza√ß√£o. O neur√¥nio receberia esses valores como entradas, e os pesos determinariam qu√£o importante cada uma dessas caracter√≠sticas √© para prever o pre√ßo.\n",
        "\n",
        "Matematicamente, a combina√ß√£o das entradas e pesos √© expressa como uma soma ponderada:\n",
        "\\[\n",
        "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "\\]\n",
        "Aqui, \\(b\\) representa o **termo de bias** (vi√©s), que √© um valor adicional que permite que o neur√¥nio se ajuste de maneira mais flex√≠vel aos dados, movendo a fun√ß√£o de ativa√ß√£o para frente ou para tr√°s no eixo de decis√£o.\n",
        "\n",
        "#### 2. **Fun√ß√£o de Soma (Weighted Sum)**\n",
        "\n",
        "O valor \\(z\\) obtido √© a soma ponderada das entradas e pesos, incluindo o bias. Esta soma ponderada √© o que o neur√¥nio processa antes de tomar qualquer \"decis√£o\" sobre a sa√≠da.\n",
        "\n",
        "A fun√ß√£o de soma √© essencial porque integra todas as entradas de forma linear, combinando-as em um √∫nico valor que representa o estado atual das informa√ß√µes processadas pelo neur√¥nio. Este valor \\(z\\) ser√° ent√£o transformado por uma fun√ß√£o de ativa√ß√£o.\n",
        "\n",
        "#### 3. **Fun√ß√£o de Ativa√ß√£o (Activation Function)**\n",
        "\n",
        "Ap√≥s calcular a soma ponderada, o neur√¥nio passa este valor \\(z\\) por uma fun√ß√£o de ativa√ß√£o. A fun√ß√£o de ativa√ß√£o √© crucial porque ela introduz **n√£o-linearidade** ao sistema. Sem ela, a rede neural seria equivalente a uma simples regress√£o linear, incapaz de capturar rela√ß√µes complexas nos dados.\n",
        "\n",
        "As fun√ß√µes de ativa√ß√£o mais comuns incluem:\n",
        "\n",
        "- **ReLU (Rectified Linear Unit):** \\( \\text{ReLU}(z) = \\max(0, z) \\)  \n",
        "  A ReLU √© popular porque resolve o problema do desvanecimento do gradiente (gradient vanishing) em redes profundas e √© eficiente computacionalmente. Ela transforma valores negativos em zero e mant√©m os valores positivos inalterados.\n",
        "\n",
        "- **Sigmoide:** \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\)  \n",
        "  A fun√ß√£o sigmoide √© usada principalmente em problemas de classifica√ß√£o bin√°ria. Ela mapeia a soma ponderada para um valor entre 0 e 1, interpret√°vel como uma probabilidade.\n",
        "\n",
        "- **Tanh (Tangente Hiperb√≥lica):** \\( \\text{Tanh}(z) = \\frac{2}{1 + e^{-2z}} - 1 \\)  \n",
        "  A tanh √© semelhante √† sigmoide, mas mapeia o valor de \\(z\\) para um intervalo entre -1 e 1. √â √∫til quando se quer centrar os dados em torno de zero.\n",
        "\n",
        "A sa√≠da do neur√¥nio ap√≥s a aplica√ß√£o da fun√ß√£o de ativa√ß√£o √©:\n",
        "\\[\n",
        "a = \\text{fun√ß√£o de ativa√ß√£o}(z)\n",
        "\\]\n",
        "onde \\(a\\) √© a sa√≠da do neur√¥nio, pronta para ser transmitida para a pr√≥xima camada na rede ou para gerar uma predi√ß√£o final, dependendo de onde o neur√¥nio se encontra na arquitetura da rede.\n",
        "\n",
        "#### 4. **Interpreta√ß√£o e Papel do Neur√¥nio Artificial**\n",
        "\n",
        "O neur√¥nio artificial, em ess√™ncia, toma uma decis√£o com base nas entradas que recebe e nos pesos associados a essas entradas. Ele processa as informa√ß√µes atrav√©s da fun√ß√£o de soma e decide, atrav√©s da fun√ß√£o de ativa√ß√£o, se deve \"ativar\" (ou seja, gerar uma sa√≠da significativa).\n",
        "\n",
        "Em uma rede neural, m√∫ltiplos neur√¥nios operam juntos, cada um processando suas respectivas entradas. O poder das redes neurais vem da capacidade de compor v√°rias camadas de neur√¥nios, cada uma capturando diferentes n√≠veis de abstra√ß√£o dos dados. Em camadas iniciais, os neur√¥nios podem identificar padr√µes simples (como bordas em uma imagem), enquanto camadas mais profundas identificam padr√µes complexos (como formas ou objetos inteiros).\n",
        "\n",
        "### Visualizando o Neur√¥nio Artificial\n",
        "\n",
        "Imagine o neur√¥nio como um filtro que recebe v√°rias informa√ß√µes, pesa cada uma, soma todas e decide, atrav√©s de uma fun√ß√£o matem√°tica, qual ser√° sua resposta. Se pensarmos em termos de uma rede neural complexa, cada neur√¥nio individualmente √© respons√°vel por uma pequena parte do trabalho, mas todos juntos formam um sistema poderoso capaz de aprender e fazer predi√ß√µes a partir dos dados.\n",
        "\n",
        "Essa estrutura b√°sica do neur√¥nio artificial √© o que permite que redes neurais, desde as mais simples at√© as mais profundas e complexas, sejam capazes de realizar tarefas como reconhecimento de imagem, tradu√ß√£o de linguagem e at√© mesmo conduzir carros autonomamente."
      ],
      "metadata": {
        "id": "plz-vCjPOsgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "MH1si2sIO1l6"
      }
    }
  ]
}